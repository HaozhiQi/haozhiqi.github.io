<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
	<title>Haozhi Qi's Homepage</title>

	<link href="res/css/bootstrap.min.css" rel="stylesheet">

	<style>
      .header {
        width: auto;
        max-width: auto;
        font-family: Arial;
        padding: 2rem 1rem;
      }

      a:link,a:visited
      {
        color: #0071bc;
        text-decoration: none;
      }
      a:hover {
        color: #208799;
      }

      hr {
        border: 10;
        height: 1px;
        background-image: linear-gradient(to right, rgba(0, 113, 188, 0.2), rgba(0, 0, 0, 0.1), rgba(0, 113, 188, 0.2));
      }

      .gap-20 {height:20px;}
      .gap-10 {height:10px;}
      .hgap-6perc {width: 6%;}
      .hgap-7perc {width: 7%;}

      ::selection {
        background: rgba(220, 220, 220);
	  }

      .section-div {height: 3em}
      .content-div {height: 2em}
      .small-div {height: 0.5em}

	</style>
	<!-- Bootstrap -->

	<!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
	<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
	<!--[if lt IE 9]>
	<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
	<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
	<![endif]-->


	<!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
	<!-- Include all compiled plugins (below), or include individual files as needed -->
	<script src="res/js/bootstrap.min.js"></script>
</head>

<div class="header">
	<div class="container">
		<div class="content-div"></div>
		<div class="row align-items-center">
			<div class="col-md-3 align-column-center">
				<center>
					<img class="img-fluid" src="profile.jpg"
					     style="border-radius:20px; border:1px; margin-bottom: 5px" width=85%" alt="">
				</center>
			</div>
			<div class="col-md-9 align-column-center">
				<h2> Haozhi Qi </h2>
				Email: hqi AT berkeley.edu<br>
				<a href="https://scholar.google.com/citations?user=iyVHKkcAAAAJ&hl=en" target="_blank"> Google Scholar </a>
				& <a href="https://github.com/HaozhiQi" target="_blank"> GitHub </a>
				& <a href="https://twitter.com/HaozhiQ" target="_blank"> Twitter </a>

				<div class="small-div"></div>
				I am a Ph.D. Candidate at UC Berkeley advised by <a href="https://people.eecs.berkeley.edu/~yima/"
				                                                    target="_blank">
				Prof. Yi Ma </a> and <a href="http://people.eecs.berkeley.edu/~malik" target="_blank">Prof. Jitendra Malik</a>, and
				a visiting researcher at Meta AI. My research focuses on the intersection of robotics,
				computer vision, tactile sensing, and machine learning. I am particularly interested in developing
				algorithms for generalizable and robust robot manipulation via the integration of deep learning and visuotactile sensing.
				<div class="small-div"></div>
				I obtained my Bachelor's degree from Hong Kong University of Science and Technology (HKUST) advised by <a
						href="http://www.cs.ust.hk/~cktang/" target="_blank"> Prof. Chi-Keung Tang</a>.
				I also spent two years in Microsoft Research Asia (MSRA) where I was a research intern advised by <a
						href="http://www.jifengdai.org/" target="_blank"> Dr. Jifeng Dai </a> and <a
						href="https://yichenwei.github.io/"
						target="_blank"> Dr. Yichen Wei</a>.
			</div>
		</div>
	</div>
</div>

<div class="container">
	<h3> Activities </h3>
	<li>
		Co-organize <a href="https://dex-manipulation.github.io/rss2024/" target="_blank">Workshop on Dexterous Manipulation: Design, Perception and Control</a> at RSS 2024.
	</li>
	<li>
		Co-organize <a href="http://www.touchprocessing.org/2023/" target="_blank">Workshop on Touch Processing: a new Sensing Modality for AI</a> at NeurIPS 2023.
	</li>
	<div class="content-div"></div>
</div>


<div class="container">
	<!-- publications come here -->
	<h3> Papers <font size="3" style="font-weight:normal">(*indicates equal technical contribution)</font></h3>

	<div style="color:white"> <font size="3"> Code Release Progress: 12 / 15. </font></div>
	<div class="small-div"></div>

	<!------------------------- Arxiv 24 --------------------------------->
	<div class="row align-items-center">
		<div class="col-md-3 align-column-center">
			<center>
				<video playsinline autoplay loop muted src="res/figs/hato.mp4" width="98%"
				       style="border-radius:10px;"></video>
			</center>
		</div>
		<div class="col-md-9">
			<font size="4"> <b> Learning Visuotactile Skills with Two Multifingered Hands </b></font><br>
			<font size="3">
				<a href="https://toruowo.github.io/" target="_blank"> Toru Lin</a>,
				<a href="" target="_blank"> Yu Zhang</a>*,
				<a href="https://colinqiyangli.github.io/" target="_blank"> Qiyang Li</a>*,
				<b> Haozhi Qi</b>*,
				<a href="https://scholar.google.com/citations?user=Ecy6lXwAAAAJ&hl=en" target="_blank"> Brent Yi</a>,
				<a href="https://people.eecs.berkeley.edu/~svlevine/" target="_blank"> Sergey Levine</a>,
				<a href="https://people.eecs.berkeley.edu/~malik/" target="_blank"> Jitendra Malik</a>
			</font>
			<br>
			Arxiv Preprint, 2024<br>
			<a href="https://arxiv.org/abs/2404.16823" target="_blank"> [Paper] </a>
			<a href="https://github.com/ToruOwO/hato" target="_blank"> [Code] </a>
			<a href="https://toruowo.github.io/hato/" target="_blank"> [Website] </a>
		</div>
	</div>
	<div class="content-div"></div>

	<!------------------------- Arxiv 24 --------------------------------->
	<div class="row align-items-center">
		<div class="col-md-3 align-column-center">
			<center>
				<video playsinline autoplay loop muted src="res/figs/twist24.mp4" width="98%"
				       style="border-radius:10px;"></video>
			</center>
		</div>
		<div class="col-md-9">
			<font size="4"> <b> Twisting Lids Off with Two Hands  </b></font><br>
			<font size="3">
				<a href="https://toruowo.github.io/" target="_blank"> Toru Lin</a>*,
				<a href="https://zhaohengyin.github.io/" target="_blank"> Zhao-Heng Yin</a>*,
				<b> Haozhi Qi</b>,
				<a href="https://people.eecs.berkeley.edu/~pabbeel/" target="_blank"> Pieter Abbeel</a>,
				<a href="https://people.eecs.berkeley.edu/~malik/" target="_blank"> Jitendra Malik</a>
			</font>
			<br>
			Arxiv Preprint, 2024<br>
			<a href="https://arxiv.org/abs/2403.02338" target="_blank"> [Paper] </a>
			<a href="https://toruowo.github.io/bimanual-twist/" target="_blank"> [Website] </a>
			<a href="https://www.youtube.com/watch?v=TaRva6UjrCo" target="_blank"> [Video] </a>
			<a href="https://twitter.com/ToruO_O/status/1765046694545412395" target="_blank"> [Twitter] </a>
		</div>
	</div>
	<div class="content-div"></div>

	<!------------------------- CoRL 23 --------------------------------->
	<div class="row align-items-center">
		<div class="col-md-3 align-column-center">
			<center>
				<video playsinline autoplay loop muted src="res/figs/corl23.mp4" width="98%"
				       style="border-radius:10px;"></video>
			</center>
		</div>
		<div class="col-md-9">
			<font size="4"> <b> General In-Hand Object Rotation with Vision and Touch </b></font><br>
			<font size="3">
				<b> Haozhi Qi</b>,
				<a href="https://scholar.google.com/citations?user=Ecy6lXwAAAAJ&hl=en" target="_blank"> Brent Yi</a>,
				<a href="http://www.cs.cmu.edu/~sudhars1/" target="_blank"> Sudharshan Suresh</a>,
				<a href="https://scholar.google.com/citations?user=p6DCMrQAAAAJ&hl=en" target="_blank"> Mike Lambeta</a>,
				<a href="http://people.eecs.berkeley.edu/~yima/" target="_blank"> Yi Ma</a>,
				<a href="https://www.robertocalandra.com/about/" target="_blank"> Roberto Calandra</a>,
				<a href="https://people.eecs.berkeley.edu/~malik/" target="_blank"> Jitendra Malik</a>
			</font>
			<br>
			<!-- 199 / 498 (39.9%) acceptance rate -->
			Conference on Robot Learning (<b>CoRL</b>), 2023<br>
			<a href="https://arxiv.org/abs/2309.09979" target="_blank"> [Paper] </a>
			<a href="https://haozhi.io/rotateit" target="_blank"> [Website] </a>
			<a href="https://youtu.be/Uh-ltingRzk" target="_blank"> [Video] </a>
			<a href="https://twitter.com/HaozhiQ/status/1704524466699763771" target="_blank"> [Twitter] </a>
		</div>
	</div>
	<div class="content-div"></div>


	<!------------------------- Arxiv 23 --------------------------------->
	<div class="row align-items-center">
		<div class="col-md-3 align-column-center">
			<center>
				<video playsinline autoplay loop muted src="res/figs/neural_feel.mp4" width="98%"
				       style="border-radius:10px;"></video>
			</center>
		</div>
		<div class="col-md-9">
			<font size="4"> <b> Neural feels with Neural Fields: Visuo-tactile Perception for In-Hand Manipulation </b></font><br>
			<font size="3">
				<a href="http://www.cs.cmu.edu/~sudhars1/" target="_blank"> Sudharshan Suresh</a>,
				<b> Haozhi Qi</b>,
				<a href="https://scholar.google.com/citations?user=9bt2Z5QAAAAJ&hl=en" target="_blank">Tingfan Wu</a>,
			    <a href="https://scholar.google.com/citations?user=3PJeg1wAAAAJ&hl=en">Taosha Fan</a>,
			    <a href="https://scholar.google.com/citations?user=rebEn8oAAAAJ&hl=en">Luis Pineda</a>,
			    <a href="https://scholar.google.com/citations?user=p6DCMrQAAAAJ&hl=en">Mike Lambeta</a>,
			    <a href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a>,
			    <a href="https://scholar.google.com/citations?user=DMTuJzAAAAAJ&hl=en">Mrinal Kalakrishnan</a>,
			    <a href="https://scholar.google.ch/citations?user=fA0rYxMAAAAJ&hl=en">Roberto Calandra</a>,
			    <a href="https://www.cs.cmu.edu/~kaess/">Michael Kaess</a>,
			    <a href="https://joeaortiz.github.io/">Joe Ortiz</a>,
			    <a href="https://www.mustafamukadam.com/">Mustafa Mukadam</a>
			</font>
			<br>
			Arxiv Preprint, 2023<br>
			<a href="https://arxiv.org/abs/2312.13469" target="_blank"> [Paper] </a>
			<a href="https://suddhu.github.io/neural-feels/" target="_blank"> [Website] </a>
			<a href="https://www.youtube.com/watch?v=KOHh0awhSEg" target="_blank"> [Video] </a>
		</div>
	</div>
	<div class="content-div"></div>

	<!------------------------- Arxiv --------------------------------->
	<div class="row align-items-center">
		<div class="col-md-3 align-column-center">
			<center>
				<video playsinline autoplay loop muted src="res/figs/NCF.mp4" width="100%"
				       style="border-radius:10px;"></video>
			</center>
		</div>
		<div class="col-md-9">
			<font size="4"> <b> Perceiving Extrinsic Contacts from Touch Improves Learning Insertion Policies</b></font><br>
			<font size="3">
				<a href="https://carolinahiguera.github.io/" target="_blank"> Carolina Higuera</a>,
				<a href="https://joeaortiz.github.io/" target="_blank"> Joseph Ortiz</a>,
				<b> Haozhi Qi</b>,
				<a href="https://scholar.google.com/citations?user=rebEn8oAAAAJ&hl=en" target="_blank"> Luis Pineda</a>,
				<a href="https://homes.cs.washington.edu/~bboots/" target="_blank"> Byron Boots</a>,
				<a href="https://www.mustafamukadam.com/" target="_blank"> Mustafa Mukadam</a>
			</font>
			<br>
			Arxiv Preprint, 2023<br>
			<a href="https://arxiv.org/abs/2309.16652" target="_blank"> [Paper] </a>
			<a href="https://github.com/carolinahiguera/NCF-Policy" target="_blank"> [Code] </a>
			<a href="https://www.youtube.com/watch?v=rG_xIfQ6-_k" target="_blank"> [Video] </a>
			<a href="https://x.com/carohiguerarias/status/1707849840674410642?s=20" target="_blank"> [Twitter] </a>
		</div>
	</div>
	<div class="content-div"></div>

	<!------------------------- CoRL 22 --------------------------------->
	<div class="row align-items-center">
		<div class="col-md-3 align-column-center">
			<center>
				<video playsinline autoplay loop muted src="res/figs/corl22.mp4" width="98%"
				       style="border-radius:10px;"></video>
			</center>
		</div>
		<div class="col-md-9">
			<font size="4"> <b> In-Hand Object Rotation via Rapid Motor Adaptation </b></font><br>
			<font size="3">
				<b> Haozhi Qi</b>*,
				<a href="https://ashish-kmr.github.io/" target="_blank"> Ashish Kumar</a>*,
				<a href="https://www.robertocalandra.com/about/" target="_blank"> Roberto Calandra</a>,
				<a href="http://people.eecs.berkeley.edu/~yima/" target="_blank"> Yi Ma</a>,
				<a href="https://people.eecs.berkeley.edu/~malik/" target="_blank"> Jitendra Malik</a>
			</font>
			<br>
			<!-- 197 / 504 (39.1%) acceptance rate -->
			Conference on Robot Learning (<b>CoRL</b>), 2022<br>
			<a href="https://arxiv.org/abs/2210.04887" target="_blank"> [Paper] </a>
			<a href="https://github.com/haozhiqi/hora" target="_blank"> [Code] </a>
			<a href="https://haozhi.io/hora" target="_blank"> [Website] </a>
			<a href="https://www.youtube.com/watch?v=yH0e0l-H7-8" target="_blank"> [Video] </a>
			<a href="https://twitter.com/HaozhiQ/status/1580217728693960704" target="_blank"> [Twitter] </a><br>
			<b style="color:#EF476F;"> Outstanding Demo Award </b><b> at NeurIPS 2023 Robot Learning Workshop</b>
		</div>
	</div>
	<div class="content-div"></div>

	<!------------------------- CVPR 22 --------------------------------->
	<div class="row align-items-center">
		<div class="col-md-3 align-column-center">
			<center>
				<video playsinline autoplay loop muted src="res/figs/cvpr22.mp4" width="98%"
				       style="border-radius:10px;"></video>
			</center>
		</div>
		<div class="col-md-9">
			<font size="4"> <b> Coupling Vision and Proprioception for Navigation of Legged Robots </b></font><br>
			<font size="3">
				<a href="https://markfzp.github.io/" target="_blank"> Zipeng Fu</a>*,
				<a href="https://ashish-kmr.github.io/" target="_blank"> Ashish Kumar</a>*,
				<a href="https://anag.me/" target="_blank"> Ananye Agarwal</a>,
				<b> Haozhi Qi</b>,
				<a href="https://people.eecs.berkeley.edu/~malik/" target="_blank"> Jitendra Malik</a>,
				<a href="https://www.cs.cmu.edu/~dpathak/" target="_blank"> Deepak Pathak</a>
			</font>
			<br>
			<!-- 2067 / 8161 (25.3%) acceptance rate -->
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2022 <br>
			<a href="https://arxiv.org/abs/2112.02094" target="_blank"> [Paper] </a>
			<a href="https://github.com/MarkFzp/navigation-locomotion" target="_blank"> [Code] </a>
			<a href="https://navigation-locomotion.github.io/" target="_blank"> [Website] </a>
			<a href="https://www.youtube.com/watch?v=sZVvutQUAQ4" target="_blank"> [Video] </a>
			<a href="https://twitter.com/pathak2206/status/1540357312703090695" target="_blank"> [Twitter] </a>
		</div>
	</div>
	<div class="content-div"></div>

	<!------------------------- JMLR 22 --------------------------------->
	<div class="row align-items-center">
		<div class="col-md-3">
			<img class="img-fluid " src="res/figs/mcr.png" style="border:0px solid black" alt="">
		</div>
		<div class="col-md-9">
			<font size="4"> <b> ReduNet: A White-box Deep Network from the Principle of Maximizing Rate
				Reduction </b></font><br>
			<font size="3">
				<a href="https://ryanchankh.github.io/" target="_blank"> Kwan Ho Ryan Chan</a>*,
				<a href="https://yaodongyu.github.io/" target="_blank"> Yaodong Yu</a>*,
				<a href="https://sites.google.com/view/cyou" target="_blank"> Chong You</a>*,
				<b> Haozhi Qi</b>,
				<a href="http://www.columbia.edu/~jw2966/" target="_blank"> John Wright</a>,
				<a href="http://people.eecs.berkeley.edu/~yima/" target="_blank"> Yi Ma</a><br>
				Journal of Machine Learning Research (<b>JMLR</b>), accepted in 2022 <br>
				<a href="https://arxiv.org/abs/2105.10446" target="_blank"> [Paper] </a>
				<a href="https://github.com/Ma-Lab-Berkeley/ReduNet" target="_blank"> [Code] </a>
				<a href="https://twitter.com/YiMaTweets/status/1523540536451231744" target="_blank"> [Twitter] </a>
			</font>
		</div>
	</div>
	<div class="content-div"></div>

	<!------------------------- ICLR 21 --------------------------------->
	<div class="row align-items-center">
		<div class="col-md-3">
			<div class="row align-items-center">
				<div class="col-6">
					<center><font size="2">Ours</font></center>
				</div>
				<div class="col-6">
					<center><font size="2">GT</font></center>
				</div>
			</div>
			<div class="row align-items-center">
				<div class="hgap-7perc"></div>
				<img src="res/figs/rpin1.gif" style="max-width:40%;border-radius:5px;border:1px solid #021a40;">
				<div class="hgap-6perc"></div>
				<img src="res/figs/rpin2.gif" style="max-width:40%;border-radius:5px;border:1px solid #021a40;">
				<div class="hgap-7perc"></div>
			</div>
		</div>
		<div class="col-md-9">
			<font size="4"> <b> Learning Long-term Visual Dynamics with Region Proposal Interaction Networks </b></font><br>
			<font size="3"> <b> Haozhi Qi</b>,
				<a href="https://xiaolonw.github.io/" target="_blank"> Xiaolong Wang</a>,
				<a href="https://www.cs.cmu.edu/~dpathak/" target="_blank"> Deepak Pathak</a>,
				<a href="http://people.eecs.berkeley.edu/~yima/" target="_blank"> Yi Ma</a>,
				<a href="https://people.eecs.berkeley.edu/~malik/" target="_blank"> Jitendra Malik</a><br>
				International Conference on Learning Representations (<b>ICLR</b>), 2021 <br>
				<a href="http://arxiv.org/abs/2008.02265" target="_blank"> [Paper] </a>
				<a href="https://github.com/HaozhiQi/RPIN" target="_blank"> [Code] </a>
				<a href="https://haozhi.io/RPIN" target="_blank"> [Website] </a>
				<a href="https://twitter.com/HaozhiQ/status/1291771371719024647" target="_blank"> [Twitter] </a>
			</font>
		</div>
	</div>
	<div class="content-div"></div>

	<!------------------------- ICML 20 --------------------------------->
	<div class="row align-items-center">
		<div class="col-md-3">
			<img class="img-fluid " src="res/figs/icml20.png" style="border:0px solid black" alt="">
		</div>
		<div class="col-md-9">
			<font size="4"> <b> Deep Isometric Learning for Visual Recognition </b></font><br>
			<font size="3"> <b> Haozhi Qi</b>,
				<a href="https://sites.google.com/view/cyou" target="_blank"> Chong You</a>,
				<a href="https://xiaolonw.github.io/" target="_blank"> Xiaolong Wang</a>,
				<a href="http://people.eecs.berkeley.edu/~yima/" target="_blank"> Yi Ma</a>,
				<a href="https://people.eecs.berkeley.edu/~malik/" target="_blank"> Jitendra Malik</a><br>
				International Conference on Machine Learning (<b>ICML</b>), 2020 <br>
				<a href="https://arxiv.org/abs/2006.16992" target="_blank"> [Paper] </a>
				<a href="https://github.com/HaozhiQi/ISONet" target="_blank"> [Code] </a>
				<a href="https://haozhi.io/ISONet" target="_blank"> [Website] </a>
				<a href="https://youtu.be/EmrVtAx8cNc" target="_blank"> [Video] </a>
				<a href="https://twitter.com/xiaolonw/status/1278148208317706240"> [Twitter] </a>
			</font>
		</div>
	</div>
	<div class="content-div"></div>
	<!------------------------- NIPS 19 --------------------------------->
	<div class="row align-items-center">
		<div class="col-md-3">
			<img class="img-fluid" src="res/figs/nips19.png" style="border:0px solid black" alt="">
		</div>
		<div class="col-md-9">
			<font size="4"> <b> NeurVPS: Neural Vanishing Point Scanning via Conic Convolution </b></font><br>
			<font size="3">
				<a href="https://yichaozhou.com" target="_blank"> Yichao Zhou</a>,
				<b> Haozhi Qi</b>,
				<a href="https://cs.stanford.edu/people/jingweih/" target="_blank"> Jingwei Huang</a>,
				<a href="http://people.eecs.berkeley.edu/~yima/" target="_blank"> Yi Ma</a><br>
				Neural Information Processing System (<b>NIPS</b>), 2019 <br>
				<a href="https://arxiv.org/abs/1910.06316" target="_blank"> [Paper] </a>
				<a href="https://github.com/zhou13/neurvps" target="_blank"> [Code] </a><br>
			</font>
		</div>
	</div>
	<div class="content-div"></div>
	<!------------------------- ICCV 19 --------------------------------->
	<div class="row align-items-center">
		<div class="col-md-3">
			<img class="img-fluid" src="res/figs/iccv19a.png" style="border:0px solid black" alt="">
		</div>
		<div class="col-md-9">
			<font size="4"> <b> Learning to Reconstruct 3D Manhattan Wireframes from a Single Image </b></font><br>
			<font size="3">
				<a href="https://yichaozhou.com" target="_blank"> Yichao Zhou</a>,
				<b> Haozhi Qi</b>,
				<a href="https://yx-s-z.github.io/" target="_blank"> Yuexiang Zhai</a>,
				<a href="https://qisun.me/" target="_blank"> Qi Sun</a>,
				<a href="http://www.zhilichen.com/" target="_blank"> Zhili Chen</a>,
				<a href="https://www.liyiwei.org/" target="_blank"> Li-Yi Wei</a>,
				<a href="http://people.eecs.berkeley.edu/~yima/" target="_blank"> Yi Ma</a><br>
				International Conference on Computer Vision (<b>ICCV</b>), 2019 (<b>Oral, 4.3% acceptance rate</b>) <br>
				<a href="https://arxiv.org/abs/1905.07482" target="_blank"> [Paper] </a> <a
						href="https://github.com/zhou13/shapeunity" target="_blank"> [Code] </a><br>
			</font>
		</div>
	</div>
	<div class="content-div"></div>
	<!------------------------- ICCV 19 --------------------------------->
	<div class="row align-items-center">
		<div class="col-md-3">
			<img class="img-fluid" src="res/figs/iccv19b.png" style="border:0px solid black" alt="">
		</div>
		<div class="col-md-9">
			<font size="4"> <b> End-to-End Wireframe Parsing </b></font> <br>
			<font size="3">
				<a href="https://yichaozhou.com" target="_blank"> Yichao Zhou</a>,
				<b> Haozhi Qi</b>,
				<a href="http://people.eecs.berkeley.edu/~yima/" target="_blank"> Yi Ma</a><br>
				International Conference on Computer Vision (<b>ICCV</b>), 2019 <br>
				<a href="https://arxiv.org/abs/1905.03246" target="_blank"> [Paper] </a> <a
						href="https://github.com/zhou13/lcnn" target="_blank"> [Code] </a><br>
			</font>
		</div>
	</div>
	<div class="content-div"></div>

	<!------------------------- ICCV 17 --------------------------------->
	<div class="row align-items-center">
		<div class="col-md-3">
			<img class="img-fluid" src="res/figs/iccv17.png" style="border:0px solid black" alt="">
		</div>
		<div class="col-md-9">
			<font size="4"> <b>Deformable Convolutional Networks </b></font><br>
			<font size="3">
				<a href="https://jifengdai.org/" target="_blank"> Jifeng Dai</a>*,
				<b>Haozhi Qi</b>*,
				<a href="http://yuwenxiong.com/" target="_blank"> Yuwen Xiong</a>*,
				<a href="https://liyi14.github.io/" target="_blank"> Yi Li</a>*,
				<a href="http://www.cs.toronto.edu/~gdzhang/" target="_blank"> Guodong Zhang</a>*,
				<a href="https://ancientmooner.github.io/" target="_blank"> Han Hu</a>,
				<a href="https://yichenwei.github.io/" target="_blank"> Yichen Wei</a><br>
				International Conference on Computer Vision (<b>ICCV</b>), 2017 (<b>Oral, 2.1% acceptance rate</b>) <br>
				<a href="https://arxiv.org/abs/1703.06211" target="_blank"> [Paper] </a>
				<a href="https://github.com/msracver/Deformable-ConvNets" target="_blank"> [Code] </a> <br>
			</font>
		</div>
	</div>
	<div class="content-div"></div>
	<!------------------------- CVPR 17 --------------------------------->
	<div class="row align-items-center">
		<div class="col-md-3">
			<img class="img-fluid" src="res/figs/cvpr17.png" style="border:0px solid black" alt="">
		</div>
		<div class="col-md-9">
			<font size="4"><b>Fully Convolutional Instance-aware Semantic Segmentation</b></font> <br>
			<font size="3">
				<a href="https://liyi14.github.io/" target="_blank"> Yi Li</a>*,
				<b>Haozhi Qi</b>*,
				<a href="https://jifengdai.org/" target="_blank"> Jifeng Dai</a>,
				<a href="https://www.researchgate.net/scientific-contributions/Xiangyang-Ji-11421572" target="_blank">
					Xiangyang Ji</a>,
				<a href="https://yichenwei.github.io/" target="_blank"> Yichen Wei</a><br>
				Computer Vision and Pattern Recognition (<b>CVPR</b>), 2017 (<b>Spotlight, 8.0% acceptance rate</b>)
				<br>
				<a href="https://arxiv.org/abs/1611.07709" target="_blank"> [Paper] </a>
				<a href="https://github.com/msracver/FCIS" target="_blank"> [Code] </a>
			</font>
		</div>
	</div>

</div>

<div class="section-div"></div>

</html>

<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
	<title>Haozhi Qi's Homepage</title>

	<link href="res/css/bootstrap.min.css" rel="stylesheet">

	<style>
      .header {
        width: auto;
        max-width: auto;
        font-family: Arial;
        padding: 2rem 1rem;
      }

      a:link,a:visited
      {
        color: #0071bc;
        text-decoration: none;
      }
      a:hover {
        color: #208799;
      }

      hr {
        border: 10;
        height: 1px;
        background-image: linear-gradient(to right, rgba(0, 113, 188, 0.2), rgba(0, 0, 0, 0.1), rgba(0, 113, 188, 0.2));
      }

      .gap-20 {height:20px;}
      .gap-10 {height:10px;}
      .hgap-6perc {width: 6%;}
      .hgap-7perc {width: 7%;}

      ::selection {
        background: rgba(220, 220, 220);
	  }

      .section-div {height: 3em}
      .content-div {height: 2em}
      .small-div {height: 0.5em}

	</style>
	<!-- Bootstrap -->

	<!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
	<script>
    document.addEventListener("DOMContentLoaded", function() {
        const videos = document.querySelectorAll("video[data-src]");
        const observer = new IntersectionObserver((entries) => {
            entries.forEach(entry => {
                if (entry.isIntersecting) {
                    const video = entry.target;
                    video.src = video.getAttribute("data-src");
                    video.autoplay = true;
                    video.loop = true;
                    observer.unobserve(video);
                }
            });
        }, { threshold: 0.1 });

        videos.forEach(video => observer.observe(video));
    });
	</script>
	<!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
	<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
	<!--[if lt IE 9]>
	<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
	<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
	<![endif]-->


	<!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
	<!-- Include all compiled plugins (below), or include individual files as needed -->
	<script src="res/js/bootstrap.min.js"></script>
</head>

<div class="header">
	<div class="container">
		<div class="content-div"></div>
		<div class="row align-items-center">
			<div class="col-md-3 align-column-center">
				<center>
					<img class="img-fluid" src="profile.jpg"
					     style="border-radius:20px; border:1px; margin-bottom: 5px" width=85%" alt="">
				</center>
			</div>
			<div class="col-md-9 align-column-center">
				<h2> Haozhi Qi </h2>
				Email: hqi AT berkeley.edu<br>
				<a href="./cv.pdf"> CV </a> &
				<a href="https://scholar.google.com/citations?user=iyVHKkcAAAAJ&hl=en" target="_blank"> Google Scholar </a>
				& <a href="https://github.com/HaozhiQi" target="_blank"> GitHub </a>
				& <a href="https://twitter.com/HaozhiQ" target="_blank"> Twitter </a>

				<div class="small-div"></div>
				I am a Ph.D. Candidate at UC Berkeley advised by <a href="https://people.eecs.berkeley.edu/~yima/"
				                                                    target="_blank">
				Prof. Yi Ma </a> and <a href="http://people.eecs.berkeley.edu/~malik" target="_blank">Prof. Jitendra Malik</a>, and
				a visiting researcher at Meta FAIR. My research lies at the intersection of robotics, computer vision, tactile sensing, and machine learning.
				<div class="small-div"></div>
				I received my Bachelor's degree from Hong Kong University of Science and Technology (HKUST), where I was advised by <a
						href="http://www.cs.ust.hk/~cktang/" target="_blank"> Prof. Chi-Keung Tang</a>.
				I also spent two years at Microsoft Research Asia (MSRA), working on computer vision with <a
						href="http://www.jifengdai.org/" target="_blank"> Dr. Jifeng Dai </a> and <a
						href="https://yichenwei.github.io/"
						target="_blank"> Dr. Yichen Wei</a>.
			</div>
		</div>
	</div>
</div>

<div class="container">
  <h3> Activities </h3>
  <ul id="activity-list">
	<li>
		Co-organize workshop on <a href="https://dex-manipulation.github.io/corl2025/" target="_blank">Dexterous Manipulation: Learning and Control with Diverse Modalities</a> at CoRL 2025.
	</li>
	<li>
		Co-organize workshop on <a href="https://dex-manipulation.github.io/rss2025/" target="_blank">Dexterous Manipulation: Learning and Control with Diverse Data</a> at RSS 2025
		<a href="https://www.youtube.com/watch?v=7a5HYjQ4wJo" target="_blank"> (Videos)</a>.
	</li>
    <li>
      Co-organize workshop on <a href="https://sites.google.com/view/dexterity-workshop-icra2025/home">Handy Moves: Dexterity in Multi-Fingered Hands</a> at ICRA 2025
      <a href="https://www.youtube.com/watch?v=cUKNbwqElCw"> (Videos)</a>.
    </li>
    <li class="toggle-item">
      Co-organize workshop on <a href="http://www.touchprocessing.org/2024/">Touch Processing: From Data to Knowledge</a> at NeurIPS 2024
      <a href="https://neurips.cc/virtual/2024/workshop/84751"> (Videos)</a>.
    </li>
    <li class="toggle-item">
      Co-organize workshop on <a href="https://dex-manipulation.github.io/corl2024/">Learning Robot Fine and Dexterous Manipulation</a> at CoRL 2024
      <a href="https://www.youtube.com/watch?v=rtHeKZJEzSg"> (Videos)</a>.
    </li>
    <li class="toggle-item">
      Co-organize workshop on <a href="https://dex-manipulation.github.io/rss2024/">Dexterous Manipulation: Design, Perception and Control</a> at RSS 2024
      <a href="https://www.youtube.com/playlist?list=PLcFTgk4zQOSy0p-TQEEMXaGi918fmUxwA"> (Videos)</a>.
    </li>
    <li class="toggle-item">
      Co-organize workshop on <a href="http://www.touchprocessing.org/2023/">Touch Processing: a new Sensing Modality for AI</a> at NeurIPS 2023
      <a href="https://neurips.cc/virtual/2023/workshop/66515"> (Videos)</a>.
    </li>
  <li class="no-bullet" id="toggle-li">
    <a href="javascript:void(0)" id="toggle-button" onclick="toggleActivities()">Show more...</a>
  </li>
  </ul>
</div>

<style>
  .toggle-item {
    display: none;
  }
  .no-bullet {
  list-style: none;
  margin-left: 0em; /* optional: align nicely with bullet text */
}
  #toggle-button {
    display: inline-block;
    margin-top: 0px;
    color: #0056b3;
    cursor: pointer;
    font-weight: 500;
  }
</style>

<script>
  function toggleActivities() {
    const hiddenItems = document.querySelectorAll('.toggle-item');
    const button = document.getElementById('toggle-button');
    const isHidden = hiddenItems[0].style.display === 'none' || hiddenItems[0].style.display === '';

    hiddenItems.forEach(item => {
      item.style.display = isHidden ? 'list-item' : 'none';
    });

    button.textContent = isHidden ? 'Show less...' : 'Show more...';
  }
</script>


<div class="container">
	<!-- publications come here -->
	<h3> Papers <font size="3" style="font-weight:normal">(*indicates equal technical contribution)</font></h3>

	<!---- Last Count is Jul 10 ---->
	<div style="color:white"> <font size="3"> Code Release Progress: 18 / 20 (90.0%). </font></div>
	<div class="small-div"></div>

	<!------------------------- arXiv 25 --------------------------------->
	<div class="row align-items-center">
		<div class="col-md-3 align-column-center">
			<center>
				<video playsinline autoplay loop muted data-src="res/figs/dexhier.mp4" width="100%" preload="metadata"
				       style="border-radius:10px;"></video>
			</center>
		</div>
		<div class="col-md-9">
			<font size="4"> <b> From Simple to Complex Skills: The Case of In-Hand Object Reorientation </b></font><br>
			<font size="3">
				<b> Haozhi Qi</b>,
				<a href="https://scholar.google.com/citations?user=Ecy6lXwAAAAJ&hl=en" target="_blank"> Brent Yi</a>,
				<a href="https://scholar.google.com/citations?user=p6DCMrQAAAAJ&hl=en" target="_blank"> Mike Lambeta</a>,
				<a href="http://people.eecs.berkeley.edu/~yima/" target="_blank"> Yi Ma</a>,
				<a href="https://www.robertocalandra.com/about/" target="_blank"> Roberto Calandra</a>,
				<a href="https://people.eecs.berkeley.edu/~malik/" target="_blank"> Jitendra Malik</a>
			</font>
			<br>
			International Conference on Robotics and Automation (<b>ICRA</b>), 2025<br>
			<a href="http://arxiv.org/abs/2501.05439" target="_blank"> [Paper] </a>
			<a href="https://dexhier.github.io/" target="_blank"> [Website] </a>
		</div>
	</div>
	<div class="content-div"></div>

	<!------------------------- ICRA 25 --------------------------------->
	<div class="row align-items-center">
		<div class="col-md-3 align-column-center">
			<center>
				<video playsinline autoplay loop muted data-src="res/figs/hato.mp4" width="98%" preload="metadata"
				       style="border-radius:10px;"></video>
			</center>
		</div>
		<div class="col-md-9">
			<font size="4"> <b> Learning Visuotactile Skills with Two Multifingered Hands </b></font><br>
			<font size="3">
				<a href="https://toruowo.github.io/" target="_blank"> Toru Lin</a>,
				<a href="" target="_blank"> Yu Zhang</a>*,
				<a href="https://colinqiyangli.github.io/" target="_blank"> Qiyang Li</a>*,
				<b> Haozhi Qi</b>*,
				<a href="https://scholar.google.com/citations?user=Ecy6lXwAAAAJ&hl=en" target="_blank"> Brent Yi</a>,
				<a href="https://people.eecs.berkeley.edu/~svlevine/" target="_blank"> Sergey Levine</a>,
				<a href="https://people.eecs.berkeley.edu/~malik/" target="_blank"> Jitendra Malik</a>
			</font>
			<br>
			International Conference on Robotics and Automation (<b>ICRA</b>), 2025<br>
			<a href="https://arxiv.org/abs/2404.16823" target="_blank"> [Paper] </a>
			<a href="https://github.com/ToruOwO/hato" target="_blank"> [Code] </a>
			<a href="https://toruowo.github.io/hato/" target="_blank"> [Website] </a>
			<a href="https://twitter.com/ToruO_O/status/1783892120631484919" target="_blank"> [Summary] </a>
		</div>
	</div>
	<div class="content-div"></div>

	<!------------------------- ICRA 25 --------------------------------->
	<div class="row align-items-center">
		<div class="col-md-3 align-column-center">
			<center>
				<video playsinline autoplay loop muted data-src="res/figs/iht.mp4" width="100%" preload="metadata"
				       style="border-radius:10px;"></video>
			</center>
		</div>
		<div class="col-md-9">
			<font size="4"> <b> Learning In-Hand Translation Using Tactile Skin With Shear and Normal Force Sensing </b></font><br>
			<font size="3">
				<a href="https://jessicayin.com/" target="_blank"> Jessica Yin</a>,
				<b> Haozhi Qi</b>,
				<a href="https://people.eecs.berkeley.edu/~malik/" target="_blank"> Jitendra Malik</a>,
				<a href="https://pikulgroup.engr.wisc.edu/" target="_blank"> James Pikul</a>,
				<a href="https://www.modlabupenn.org/" target="_blank"> Mark Yim</a>,
				<a href="https://tesshellebrekers.com/" target="_blank"> Tess Hellebrekers</a>
			</font>
			<br>
			International Conference on Robotics and Automation (<b>ICRA</b>), 2025<br>
			<a href="https://arxiv.org/abs/2407.07885" target="_blank"> [Paper] </a>
			<a href="https://github.com/jessicayin/tactile_skin_model" target="_blank"> [Code] </a>
			<a href="https://jessicayin.github.io/tactile-skin-rl/" target="_blank"> [Website] </a>
			<a href="https://twitter.com/HaozhiQ/status/1811798745467748616" target="_blank"> [Summary] </a>
		</div>
	</div>
	<div class="content-div"></div>

	<!------------------------- ICRA 25 --------------------------------->
	<div class="row align-items-center">
		<div class="col-md-3 align-column-center">
			<center>
				<video playsinline autoplay loop muted data-src="res/figs/hop.mp4" width="100%" preload="metadata"
				       style="border-radius:10px;"></video>
			</center>
		</div>
		<div class="col-md-9">
			<font size="4"> <b> Hand-Object Interaction Pretraining from Videos </b></font><br>
			<font size="3">
				<a href="https://hgaurav2k.github.io/" target="_blank"> Himanshu Gaurav Singh</a>*,
				<a href="https://antonilo.github.io/" target="_blank"> Antonio Loquercio</a>*,
				<a href="https://sferrazza.cc/" target="_blank"> Carlo Sferrazza</a>,
				<a href="https://janehwu.github.io/" target="_blank"> Jane Wu</a>,
				<b> Haozhi Qi</b>,
				<a href="https://people.eecs.berkeley.edu/~pabbeel/" target="_blank"> Pieter Abbeel</a>,
				<a href="https://people.eecs.berkeley.edu/~malik/" target="_blank"> Jitendra Malik</a>
			</font>
			<br>
			International Conference on Robotics and Automation (<b>ICRA</b>), 2025<br>
			<a href="https://arxiv.org/abs/2409.08273" target="_blank"> [Paper] </a>
			<a href="https://github.com/hgaurav2k/hop" target="_blank"> [Code] </a>
			<a href="https://hgaurav2k.github.io/hop/" target="_blank"> [Website] </a>
			<a href="https://twitter.com/Cinnabar233/status/1836471176237072886" target="_blank"> [Summary] </a>
		</div>
	</div>
	<div class="content-div"></div>

	<!------------------------- CoRL 24 --------------------------------->
	<div class="row align-items-center">
		<div class="col-md-3 align-column-center">
			<center>
				<video playsinline autoplay loop muted data-src="res/figs/penspin.mp4" width="100%" preload="metadata"
				       style="border-radius:10px;"></video>
			</center>
		</div>
		<div class="col-md-9">
			<font size="4"> <b> Lessons from Learning to Spin “Pens” </b></font><br>
			<font size="3">
				<a href="https://wang59695487.github.io/" target="_blank">Jun Wang</a>*,
				<a href="https://yingyuan0414.github.io/" target="_blank">Ying Yuan</a>*,
				<a href="https://www.linkedin.com/in/haichuan-che-7338721b1/" target="_blank">Haichuan Che</a>*,
				<b> Haozhi Qi</b>*,
				<a href="https://people.eecs.berkeley.edu/~yima/" target="_blank">Yi Ma</a>,
				<a href="https://people.eecs.berkeley.edu/~malik/" target="_blank">Jitendra Malik</a>,
				<a href="https://xiaolonw.github.io/" target="_blank">Xiaolong Wang</a>
			</font>
			<br>
			Conference on Robot Learning (<b>CoRL</b>), 2024<br>
			<a href="https://arxiv.org/abs/2407.18902" target="_blank"> [Paper] </a>
			<a href="https://github.com/HaozhiQi/penspin/" target="_blank"> [Code] </a>
			<a href="https://penspin.github.io/" target="_blank"> [Website] </a>
			<a href="https://twitter.com/HaozhiQ/status/1817949548813152763" target="_blank"> [Summary] </a>
		</div>
	</div>
	<div class="content-div"></div>

	<!------------------------- CoRL 24 --------------------------------->
	<div class="row align-items-center">
		<div class="col-md-3 align-column-center">
			<center>
				<video playsinline autoplay loop muted data-src="res/figs/twist24.mp4" width="98%" preload="metadata"
				       style="border-radius:10px;"></video>
			</center>
		</div>
		<div class="col-md-9">
			<font size="4"> <b> Twisting Lids Off with Two Hands  </b></font><br>
			<font size="3">
				<a href="https://toruowo.github.io/" target="_blank"> Toru Lin</a>*,
				<a href="https://zhaohengyin.github.io/" target="_blank"> Zhao-Heng Yin</a>*,
				<b> Haozhi Qi</b>,
				<a href="https://people.eecs.berkeley.edu/~pabbeel/" target="_blank"> Pieter Abbeel</a>,
				<a href="https://people.eecs.berkeley.edu/~malik/" target="_blank"> Jitendra Malik</a>
			</font>
			<br>
			Conference on Robot Learning (<b>CoRL</b>), 2024<br>
			<a href="https://arxiv.org/abs/2403.02338" target="_blank"> [Paper] </a>
			<a href="https://github.com/ToruOwO/twisting-lids" target="_blank"> [Code] </a>
			<a href="https://toruowo.github.io/bimanual-twist/" target="_blank"> [Website] </a>
			<a href="https://www.youtube.com/watch?v=TaRva6UjrCo" target="_blank"> [Video] </a>
			<a href="https://twitter.com/ToruO_O/status/1765046694545412395" target="_blank"> [Summary] </a>
		</div>
	</div>
	<div class="content-div"></div>

	<!------------------------- Arxiv 23 --------------------------------->
	<div class="row align-items-center">
		<div class="col-md-3 align-column-center">
			<center>
				<video playsinline autoplay loop muted data-src="res/figs/neural_feel.mp4" width="98%" preload="metadata"
				       style="border-radius:10px;"></video>
			</center>
		</div>
		<div class="col-md-9">
			<font size="4"> <b> Neural feels with Neural Fields: Visuo-tactile Perception for In-Hand Manipulation </b></font><br>
			<font size="3">
				<a href="http://www.cs.cmu.edu/~sudhars1/" target="_blank"> Sudharshan Suresh</a>,
				<b> Haozhi Qi</b>,
				<a href="https://scholar.google.com/citations?user=9bt2Z5QAAAAJ&hl=en" target="_blank">Tingfan Wu</a>,
			    <a href="https://scholar.google.com/citations?user=3PJeg1wAAAAJ&hl=en">Taosha Fan</a>,
			    <a href="https://scholar.google.com/citations?user=rebEn8oAAAAJ&hl=en">Luis Pineda</a>,
			    <a href="https://scholar.google.com/citations?user=p6DCMrQAAAAJ&hl=en">Mike Lambeta</a>,
			    <a href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a>,
			    <a href="https://scholar.google.com/citations?user=DMTuJzAAAAAJ&hl=en">Mrinal Kalakrishnan</a>,
			    <a href="https://scholar.google.ch/citations?user=fA0rYxMAAAAJ&hl=en">Roberto Calandra</a>,
			    <a href="https://www.cs.cmu.edu/~kaess/">Michael Kaess</a>,
			    <a href="https://joeaortiz.github.io/">Joseph Ortiz</a>,
			    <a href="https://www.mustafamukadam.com/">Mustafa Mukadam</a>
			</font>
			<br>
			<b>Science Robotics</b>, Nov 2024 (<b>Journal Cover</b>)<br>
			<a href="https://www.science.org/doi/10.1126/scirobotics.adl0628" target="_blank"> [Paper] </a>
			<a href="https://github.com/facebookresearch/neuralfeels" target="_blank"> [Code] </a>
			<a href="https://suddhu.github.io/neural-feels/" target="_blank"> [Website] </a>
			<a href="https://www.youtube.com/watch?v=KOHh0awhSEg" target="_blank"> [Video] </a>
		</div>
	</div>
	<div class="content-div"></div>

	<!------------------------- Digit --------------------------------->
	<div class="row align-items-center">
		<div class="col-md-3 align-column-center">
			<img class="img-fluid" src="res/figs/digit360.gif" style="border:0px solid black" alt="">
		</div>
		<div class="col-md-9">
			<font size="4"> <b> Digitizing Touch with an Artificial Multimodal Fingertip  </b></font><br>
			<font size="3">
				<a href="https://scholar.google.com/citations?user=p6DCMrQAAAAJ&hl=en" target="_blank"> Mike Lambeta</a>,
				<a href="https://scholar.google.com/citations?user=9bt2Z5QAAAAJ&hl=en" target="_blank">Tingfan Wu</a>,
				Ali Sengül,
				Victoria Rose Most,
				Nolan Black,
				Kevin Sawyer,
				Romeo Mercado,
				<b> Haozhi&nbsp;Qi</b>,
				Alexander Sohn,
				Byron Taylor,
				Norb Tydingco,
				Gregg Kammerer,
				Dave Stroud,
				Jake Khatha,
				Kurt Jenkins,
				Kyle Most,
				Neal Stein,
				Ricardo Chavira,
				Thomas Craven-Bartle,
				Eric Sanchez,
				Yitian Ding,
				<a href="https://people.eecs.berkeley.edu/~malik/" target="_blank"> Jitendra Malik</a>,
				<a href="https://www.robertocalandra.com/about/" target="_blank"> Roberto Calandra</a>
			</font>
			<br>
			Tech Report, Nov 2024<br>
			<a href="https://arxiv.org/abs/2411.02479/" target="_blank"> [Paper] </a>
			<a href="https://github.com/facebookresearch/digit360/" target="_blank"> [Code] </a>
			<a href="https://digit.ml/" target="_blank"> [Website] </a>
			
		</div>
	</div>
	<div class="content-div"></div>

	<!------------------------- CoRL 23 --------------------------------->
	<div class="row align-items-center">
		<div class="col-md-3 align-column-center">
			<center>
				<video playsinline autoplay loop muted src="res/figs/corl23.mp4" width="98%" preload="metadata"
				       style="border-radius:10px;"></video>
			</center>
		</div>
		<div class="col-md-9">
			<font size="4"> <b> General In-Hand Object Rotation with Vision and Touch </b></font><br>
			<font size="3">
				<b> Haozhi Qi</b>,
				<a href="https://scholar.google.com/citations?user=Ecy6lXwAAAAJ&hl=en" target="_blank"> Brent Yi</a>,
				<a href="http://www.cs.cmu.edu/~sudhars1/" target="_blank"> Sudharshan Suresh</a>,
				<a href="https://scholar.google.com/citations?user=p6DCMrQAAAAJ&hl=en" target="_blank"> Mike Lambeta</a>,
				<a href="http://people.eecs.berkeley.edu/~yima/" target="_blank"> Yi Ma</a>,
				<a href="https://www.robertocalandra.com/about/" target="_blank"> Roberto Calandra</a>,
				<a href="https://people.eecs.berkeley.edu/~malik/" target="_blank"> Jitendra Malik</a>
			</font>
			<br>
			<!-- 199 / 498 (39.9%) acceptance rate -->
			Conference on Robot Learning (<b>CoRL</b>), 2023<br>
			<a href="https://arxiv.org/abs/2309.09979" target="_blank"> [Paper] </a>
			<a href="https://haozhi.io/rotateit" target="_blank"> [Website] </a>
			<a href="https://youtu.be/Uh-ltingRzk" target="_blank"> [Video] </a>
			<a href="https://twitter.com/HaozhiQ/status/1704524466699763771" target="_blank"> [Summary] </a>
		</div>
	</div>
	<div class="content-div"></div>

	<!------------------------- Arxiv --------------------------------->
	<div class="row align-items-center">
		<div class="col-md-3 align-column-center">
			<center>
				<video playsinline autoplay loop muted data-src="res/figs/NCF.mp4" width="100%" preload="metadata"
				       style="border-radius:10px;"></video>
			</center>
		</div>
		<div class="col-md-9">
			<font size="4"> <b> Perceiving Extrinsic Contacts from Touch Improves Learning Insertion Policies</b></font><br>
			<font size="3">
				<a href="https://carolinahiguera.github.io/" target="_blank"> Carolina Higuera</a>,
				<a href="https://joeaortiz.github.io/" target="_blank"> Joseph Ortiz</a>,
				<b> Haozhi Qi</b>,
				<a href="https://scholar.google.com/citations?user=rebEn8oAAAAJ&hl=en" target="_blank"> Luis Pineda</a>,
				<a href="https://homes.cs.washington.edu/~bboots/" target="_blank"> Byron Boots</a>,
				<a href="https://www.mustafamukadam.com/" target="_blank"> Mustafa Mukadam</a>
			</font>
			<br>
			Tech Report, Sep 2023<br>
			<a href="https://arxiv.org/abs/2309.16652" target="_blank"> [Paper] </a>
			<a href="https://github.com/carolinahiguera/NCF-Policy" target="_blank"> [Code] </a>
			<a href="https://www.youtube.com/watch?v=rG_xIfQ6-_k" target="_blank"> [Video] </a>
			<a href="https://x.com/carohiguerarias/status/1707849840674410642?s=20" target="_blank"> [Summary] </a>
		</div>
	</div>
	<div class="content-div"></div>

	<!------------------------- CoRL 22 --------------------------------->
	<div class="row align-items-center">
		<div class="col-md-3 align-column-center">
			<center>
				<video playsinline autoplay loop muted data-src="res/figs/corl22.mp4" width="98%" preload="metadata"
				       style="border-radius:10px;"></video>
			</center>
		</div>
		<div class="col-md-9">
			<font size="4"> <b> In-Hand Object Rotation via Rapid Motor Adaptation </b></font><br>
			<font size="3">
				<b> Haozhi Qi</b>*,
				<a href="https://ashish-kmr.github.io/" target="_blank"> Ashish Kumar</a>*,
				<a href="https://www.robertocalandra.com/about/" target="_blank"> Roberto Calandra</a>,
				<a href="http://people.eecs.berkeley.edu/~yima/" target="_blank"> Yi Ma</a>,
				<a href="https://people.eecs.berkeley.edu/~malik/" target="_blank"> Jitendra Malik</a>
			</font>
			<br>
			<!-- 197 / 504 (39.1%) acceptance rate -->
			Conference on Robot Learning (<b>CoRL</b>), 2022<br>
			<a href="https://arxiv.org/abs/2210.04887" target="_blank"> [Paper] </a>
			<a href="https://github.com/haozhiqi/hora" target="_blank"> [Code] </a>
			<a href="https://haozhi.io/hora" target="_blank"> [Website] </a>
			<a href="https://www.youtube.com/watch?v=yH0e0l-H7-8" target="_blank"> [Video] </a>
			<a href="https://twitter.com/HaozhiQ/status/1580217728693960704" target="_blank"> [Summary] </a><br>
			<b style="color:#EF476F;"> Outstanding Demo Award </b><b> at NeurIPS 2023 Robot Learning Workshop</b>
		</div>
	</div>
	<div class="content-div"></div>

	<!------------------------- CVPR 22 --------------------------------->
	<div class="row align-items-center">
		<div class="col-md-3 align-column-center">
			<center>
				<video playsinline autoplay loop muted data-src="res/figs/cvpr22.mp4" width="98%" preload="metadata"
				       style="border-radius:10px;"></video>
			</center>
		</div>
		<div class="col-md-9">
			<font size="4"> <b> Coupling Vision and Proprioception for Navigation of Legged Robots </b></font><br>
			<font size="3">
				<a href="https://markfzp.github.io/" target="_blank"> Zipeng Fu</a>*,
				<a href="https://ashish-kmr.github.io/" target="_blank"> Ashish Kumar</a>*,
				<a href="https://anag.me/" target="_blank"> Ananye Agarwal</a>,
				<b> Haozhi Qi</b>,
				<a href="https://people.eecs.berkeley.edu/~malik/" target="_blank"> Jitendra Malik</a>,
				<a href="https://www.cs.cmu.edu/~dpathak/" target="_blank"> Deepak Pathak</a>
			</font>
			<br>
			<!-- 2067 / 8161 (25.3%) acceptance rate -->
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2022 <br>
			<a href="https://arxiv.org/abs/2112.02094" target="_blank"> [Paper] </a>
			<a href="https://github.com/MarkFzp/navigation-locomotion" target="_blank"> [Code] </a>
			<a href="https://navigation-locomotion.github.io/" target="_blank"> [Website] </a>
			<a href="https://www.youtube.com/watch?v=sZVvutQUAQ4" target="_blank"> [Video] </a>
			<a href="https://twitter.com/pathak2206/status/1540357312703090695" target="_blank"> [Summary] </a>
		</div>
	</div>
	<div class="content-div"></div>

	<!------------------------- JMLR 22 --------------------------------->
	<div class="row align-items-center">
		<div class="col-md-3">
			<img class="img-fluid" src="res/figs/mcr.png" style="border:0px solid black" alt="">
		</div>
		<div class="col-md-9">
			<font size="4"> <b> ReduNet: A White-box Deep Network from the Principle of Maximizing Rate
				Reduction </b></font><br>
			<font size="3">
				<a href="https://ryanchankh.github.io/" target="_blank"> Kwan Ho Ryan Chan</a>*,
				<a href="https://yaodongyu.github.io/" target="_blank"> Yaodong Yu</a>*,
				<a href="https://sites.google.com/view/cyou" target="_blank"> Chong You</a>*,
				<b> Haozhi Qi</b>,
				<a href="http://www.columbia.edu/~jw2966/" target="_blank"> John Wright</a>,
				<a href="http://people.eecs.berkeley.edu/~yima/" target="_blank"> Yi Ma</a><br>
				Journal of Machine Learning Research (<b>JMLR</b>), accepted in 2022 <br>
				<a href="https://arxiv.org/abs/2105.10446" target="_blank"> [Paper] </a>
				<a href="https://github.com/Ma-Lab-Berkeley/ReduNet" target="_blank"> [Code] </a>
				<a href="https://twitter.com/YiMaTweets/status/1523540536451231744" target="_blank"> [Summary] </a>
			</font>
		</div>
	</div>
	<div class="content-div"></div>

	<!------------------------- ICLR 21 --------------------------------->
	<div class="row align-items-center">
		<div class="col-md-3">
			<div class="row align-items-center">
				<div class="col-6">
					<center><font size="2">Ours</font></center>
				</div>
				<div class="col-6">
					<center><font size="2">GT</font></center>
				</div>
			</div>
			<div class="row align-items-center">
				<div class="hgap-7perc"></div>
				<img src="res/figs/rpin1.gif" style="max-width:40%;border-radius:5px;border:1px solid #021a40;">
				<div class="hgap-6perc"></div>
				<img src="res/figs/rpin2.gif" style="max-width:40%;border-radius:5px;border:1px solid #021a40;">
				<div class="hgap-7perc"></div>
			</div>
		</div>
		<div class="col-md-9">
			<font size="4"> <b> Learning Long-term Visual Dynamics with Region Proposal Interaction Networks </b></font><br>
			<font size="3"> <b> Haozhi Qi</b>,
				<a href="https://xiaolonw.github.io/" target="_blank"> Xiaolong Wang</a>,
				<a href="https://www.cs.cmu.edu/~dpathak/" target="_blank"> Deepak Pathak</a>,
				<a href="http://people.eecs.berkeley.edu/~yima/" target="_blank"> Yi Ma</a>,
				<a href="https://people.eecs.berkeley.edu/~malik/" target="_blank"> Jitendra Malik</a><br>
				International Conference on Learning Representations (<b>ICLR</b>), 2021 <br>
				<a href="http://arxiv.org/abs/2008.02265" target="_blank"> [Paper] </a>
				<a href="https://github.com/HaozhiQi/RPIN" target="_blank"> [Code] </a>
				<a href="https://haozhi.io/RPIN" target="_blank"> [Website] </a>
				<a href="https://twitter.com/HaozhiQ/status/1291771371719024647" target="_blank"> [Summary] </a>
			</font>
		</div>
	</div>
	<div class="content-div"></div>

	<!------------------------- ICML 20 --------------------------------->
	<div class="row align-items-center">
		<div class="col-md-3">
			<img class="img-fluid " src="res/figs/icml20.png" style="border:0px solid black" alt="">
		</div>
		<div class="col-md-9">
			<font size="4"> <b> Deep Isometric Learning for Visual Recognition </b></font><br>
			<font size="3"> <b> Haozhi Qi</b>,
				<a href="https://sites.google.com/view/cyou" target="_blank"> Chong You</a>,
				<a href="https://xiaolonw.github.io/" target="_blank"> Xiaolong Wang</a>,
				<a href="http://people.eecs.berkeley.edu/~yima/" target="_blank"> Yi Ma</a>,
				<a href="https://people.eecs.berkeley.edu/~malik/" target="_blank"> Jitendra Malik</a><br>
				International Conference on Machine Learning (<b>ICML</b>), 2020 <br>
				<a href="https://arxiv.org/abs/2006.16992" target="_blank"> [Paper] </a>
				<a href="https://github.com/HaozhiQi/ISONet" target="_blank"> [Code] </a>
				<a href="https://haozhi.io/ISONet" target="_blank"> [Website] </a>
				<a href="https://youtu.be/EmrVtAx8cNc" target="_blank"> [Video] </a>
				<a href="https://twitter.com/xiaolonw/status/1278148208317706240"> [Summary] </a>
			</font>
		</div>
	</div>
	<div class="content-div"></div>
	<!------------------------- NIPS 19 --------------------------------->
	<div class="row align-items-center">
		<div class="col-md-3">
			<img class="img-fluid" src="res/figs/nips19.png" style="border:0px solid black" alt="">
		</div>
		<div class="col-md-9">
			<font size="4"> <b> NeurVPS: Neural Vanishing Point Scanning via Conic Convolution </b></font><br>
			<font size="3">
				<a href="https://yichaozhou.com" target="_blank"> Yichao Zhou</a>,
				<b> Haozhi Qi</b>,
				<a href="https://cs.stanford.edu/people/jingweih/" target="_blank"> Jingwei Huang</a>,
				<a href="http://people.eecs.berkeley.edu/~yima/" target="_blank"> Yi Ma</a><br>
				Neural Information Processing System (<b>NIPS</b>), 2019 <br>
				<a href="https://arxiv.org/abs/1910.06316" target="_blank"> [Paper] </a>
				<a href="https://github.com/zhou13/neurvps" target="_blank"> [Code] </a><br>
			</font>
		</div>
	</div>
	<div class="content-div"></div>
	<!------------------------- ICCV 19 --------------------------------->
	<div class="row align-items-center">
		<div class="col-md-3">
			<img class="img-fluid" src="res/figs/iccv19a.png" style="border:0px solid black" alt="">
		</div>
		<div class="col-md-9">
			<font size="4"> <b> Learning to Reconstruct 3D Manhattan Wireframes from a Single Image </b></font><br>
			<font size="3">
				<a href="https://yichaozhou.com" target="_blank"> Yichao Zhou</a>,
				<b> Haozhi Qi</b>,
				<a href="https://yx-s-z.github.io/" target="_blank"> Yuexiang Zhai</a>,
				<a href="https://qisun.me/" target="_blank"> Qi Sun</a>,
				<a href="http://www.zhilichen.com/" target="_blank"> Zhili Chen</a>,
				<a href="https://www.liyiwei.org/" target="_blank"> Li-Yi Wei</a>,
				<a href="http://people.eecs.berkeley.edu/~yima/" target="_blank"> Yi Ma</a><br>
				International Conference on Computer Vision (<b>ICCV</b>), 2019 (<b>Oral, 4.3% acceptance rate</b>) <br>
				<a href="https://arxiv.org/abs/1905.07482" target="_blank"> [Paper] </a> <a
						href="https://github.com/zhou13/shapeunity" target="_blank"> [Code] </a><br>
			</font>
		</div>
	</div>
	<div class="content-div"></div>
	<!------------------------- ICCV 19 --------------------------------->
	<div class="row align-items-center">
		<div class="col-md-3">
			<img class="img-fluid" src="res/figs/iccv19b.png" style="border:0px solid black" alt="">
		</div>
		<div class="col-md-9">
			<font size="4"> <b> End-to-End Wireframe Parsing </b></font> <br>
			<font size="3">
				<a href="https://yichaozhou.com" target="_blank"> Yichao Zhou</a>,
				<b> Haozhi Qi</b>,
				<a href="http://people.eecs.berkeley.edu/~yima/" target="_blank"> Yi Ma</a><br>
				International Conference on Computer Vision (<b>ICCV</b>), 2019 <br>
				<a href="https://arxiv.org/abs/1905.03246" target="_blank"> [Paper] </a> <a
						href="https://github.com/zhou13/lcnn" target="_blank"> [Code] </a><br>
			</font>
		</div>
	</div>
	<div class="content-div"></div>

	<!------------------------- ICCV 17 --------------------------------->
	<div class="row align-items-center">
		<div class="col-md-3">
			<img class="img-fluid" src="res/figs/iccv17.png" style="border:0px solid black" alt="">
		</div>
		<div class="col-md-9">
			<font size="4"> <b>Deformable Convolutional Networks </b></font><br>
			<font size="3">
				<a href="https://jifengdai.org/" target="_blank"> Jifeng Dai</a>*,
				<b>Haozhi Qi</b>*,
				<a href="http://yuwenxiong.com/" target="_blank"> Yuwen Xiong</a>*,
				<a href="https://liyi14.github.io/" target="_blank"> Yi Li</a>*,
				<a href="http://www.cs.toronto.edu/~gdzhang/" target="_blank"> Guodong Zhang</a>*,
				<a href="https://ancientmooner.github.io/" target="_blank"> Han Hu</a>,
				<a href="https://yichenwei.github.io/" target="_blank"> Yichen Wei</a><br>
				International Conference on Computer Vision (<b>ICCV</b>), 2017 (<b>Oral, 2.1% acceptance rate</b>) <br>
				<a href="https://arxiv.org/abs/1703.06211" target="_blank"> [Paper] </a>
				<a href="https://github.com/msracver/Deformable-ConvNets" target="_blank"> [Code] </a> <br>
			</font>
		</div>
	</div>
	<div class="content-div"></div>
	<!------------------------- CVPR 17 --------------------------------->
	<div class="row align-items-center">
		<div class="col-md-3">
			<img class="img-fluid" src="res/figs/cvpr17.png" style="border:0px solid black" alt="">
		</div>
		<div class="col-md-9">
			<font size="4"><b>Fully Convolutional Instance-aware Semantic Segmentation</b></font> <br>
			<font size="3">
				<a href="https://liyi14.github.io/" target="_blank"> Yi Li</a>*,
				<b>Haozhi Qi</b>*,
				<a href="https://jifengdai.org/" target="_blank"> Jifeng Dai</a>,
				<a href="https://www.researchgate.net/scientific-contributions/Xiangyang-Ji-11421572" target="_blank">
					Xiangyang Ji</a>,
				<a href="https://yichenwei.github.io/" target="_blank"> Yichen Wei</a><br>
				Computer Vision and Pattern Recognition (<b>CVPR</b>), 2017 (<b>Spotlight, 8.0% acceptance rate</b>)
				<br>
				<a href="https://arxiv.org/abs/1611.07709" target="_blank"> [Paper] </a>
				<a href="https://github.com/msracver/FCIS" target="_blank"> [Code] </a>
			</font>
		</div>
	</div>

</div>

<div class="section-div"></div>

</html>
